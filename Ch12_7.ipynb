{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad01ccb-7c18-4720-9ca9-1eb9d37328d8",
   "metadata": {},
   "source": [
    "## Chapter 12: Discriminant Analysis and Other Linear Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6028bd74-a9cb-4f73-8812-cc035d747bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rpy2 import robjects\n",
    "from rpy2.robjects.packages import importr, data\n",
    "import os\n",
    "import pyreadr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.gray()\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('mode.chained_assignment',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f06a69f-801b-4b6b-b1d9-e2b6f8ae85ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = importr('base')\n",
    "set_seed = robjects.r(\"set.seed\")\n",
    "APM = importr('AppliedPredictiveModeling')\n",
    "APMdatafolder = os.path.expanduser(\"~/Documents/dataset/AppliedPredictiveModeling/data\")\n",
    "print(os.path.isdir(APMdatafolder))\n",
    "unimelbdatafolder = os.path.expanduser(\"~/Documents/dataset/unimelb\")\n",
    "os.path.isdir(unimelbdatafolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c22b42-8e34-4612-9075-c7dfadf17808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "traindata_filename='unimelb_training.csv'\n",
    "trainfile_path = os.path.join(unimelbdatafolder, traindata_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20263fec-8d08-4afd-9880-11936479e4bc",
   "metadata": {},
   "source": [
    "### 1. Data clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fa53e2-04f3-4a58-bd8e-1574e30da045",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### A few utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab47b09-97aa-42e7-9b5f-ec87b8c0f6b7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleanRawData(raw0):\n",
    "    raw = raw0.copy()\n",
    "    raw['Sponsor.Code'] = raw['Sponsor.Code'].fillna('Unk')\n",
    "    raw['Sponsor.Code'] = 'Sponsor'+raw['Sponsor.Code']\n",
    "\n",
    "    raw['Grant.Category.Code'] = raw['Grant.Category.Code'].fillna('Unk')\n",
    "    raw['Grant.Category.Code'] = 'GrantCat'+raw['Grant.Category.Code']\n",
    "\n",
    "    raw['Contract.Value.Band...see.note.A'] = raw['Contract.Value.Band...see.note.A'].fillna('Unk')\n",
    "    raw['Contract.Value.Band...see.note.A'] = 'ContractValueBand'+raw['Contract.Value.Band...see.note.A']\n",
    "\n",
    "    raw['Role.1'] = raw['Role.1'].fillna('Unk')\n",
    "    return raw\n",
    "    \n",
    "def getVerticalData(raw, namesPre):\n",
    "    tmp = []\n",
    "    for i in range(1,16):\n",
    "        tmpData = pd.DataFrame()\n",
    "        tmpData['Grant.Application.ID'] = raw['Grant.Application.ID']\n",
    "        for x in namesPre:\n",
    "            if x+'.'+str(i) in raw.columns:\n",
    "                tmpData[x] = raw[x+'.'+str(i)]\n",
    "        tmp.append(tmpData)\n",
    "    vertical = pd.concat(tmp)\n",
    "    vertical = vertical[vertical['Role'].notnull()]\n",
    "    return vertical\n",
    "\n",
    "def cleanVerticalData(v0):\n",
    "    v = v0.copy()\n",
    "    v.loc[v['Country.of.Birth'].notnull(),'Country.of.Birth'] = \\\n",
    "        v[v['Country.of.Birth'].notnull()]['Country.of.Birth'].apply(lambda x: x.replace(\" \",\"\"))\n",
    "    \n",
    "    v['Home.Language'] = v['Home.Language'].apply(lambda x: 'OtherLang' if x=='Other' else x)\n",
    "    v['Dept.No.'] = v['Dept.No.'].apply(lambda x: 'Dept'+str(int(x)) if pd.notna(x) else 'DeptNA')\n",
    "    v['Faculty.No.'] = v['Faculty.No.'].apply(lambda x: 'Faculty'+str(int(x)) if pd.notna(x) else 'FacultyNA')\n",
    "    \n",
    "    v['RFCD.Code'] = v['RFCD.Code'].apply(lambda x: 'RFCD'+str(int(x)) if pd.notna(x) else 'RFCDNA')\n",
    "    v['RFCD.Code'] = v['RFCD.Code'].apply(lambda x: 'RFCDNA' if x in ['RFCD0','RFCD999999'] else x)\n",
    "    v.loc[v['RFCD.Code'].isin(['RFCDNA']),'RFCD.Percentage'] = None\n",
    "    \n",
    "    v['SEO.Code'] = v['SEO.Code'].apply(lambda x: 'SEO'+str(int(x)) if pd.notna(x) else 'SEONA')\n",
    "    v['SEO.Code'] = v['SEO.Code'].apply(lambda x: 'SEONA' if x in ['SEO0','SEO999999'] else x)\n",
    "    v.loc[v['SEO.Code'].isin(['SEONA']),'SEO.Percentage'] = None\n",
    "    \n",
    "    colName = 'No..of.Years.in.Uni.at.Time.of.Grant'\n",
    "    v[colName] = v[colName].map({'>=0 to 5':'Duration0to5',\n",
    "                                 '>5 to 10':'Duration5to10',\n",
    "                                 '>10 to 15':'Duration10to15',\n",
    "                                 'more than 15':'DurationGT15',\n",
    "                                 'Less than 0':'DurationLT0'},\n",
    "                           na_action='ignore')\n",
    "    v[colName] = v[colName].fillna('DurationUnk')\n",
    "    \n",
    "    return v\n",
    "\n",
    "def noZV(w): \n",
    "    dropColumns = []\n",
    "    for c in w.columns:\n",
    "        if len(w[c].drop_duplicates())==1:\n",
    "            dropColumns.append(c)\n",
    "    return w.drop(columns = dropColumns)\n",
    "    \n",
    "def getSummaryData(v):\n",
    "    people, totalPub, investPub, investDuration, investFaculty, investDept, investGrants, \\\n",
    "        investPhD, investLang, investCountry, investDOB, investCount, grantData, SEOcount, RFCDcount \\\n",
    "        = pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(),\\\n",
    "        pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(),\\\n",
    "        pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    allID = v[['Grant.Application.ID']].drop_duplicates().sort_values('Grant.Application.ID')\n",
    "    \n",
    "    shortNames = {\"EXT_CHIEF_INVESTIGATOR\":\"ECI\", \"STUD_CHIEF_INVESTIGATOR\":\"SCI\", \"CHIEF_INVESTIGATOR\":\"CI\",\\\n",
    "                 \"DELEGATED_RESEARCHER\":\"DR\", \"EXTERNAL_ADVISOR\":\"EA\", \"HONVISIT\":\"HV\",\\\n",
    "                 \"PRINCIPAL_SUPERVISOR\":\"PS\", \"STUDRES\":\"SR\", \"Unk\":\"UNK\", \"\":\"\"}\n",
    "    \n",
    "    # calculate the number of people per Grant application\n",
    "    w = v.groupby('Grant.Application.ID').agg(numPeople = ('Grant.Application.ID','count')).reset_index()\n",
    "    people = noZV(allID.merge(w, on = ['Grant.Application.ID'], how = 'left'))\n",
    "    \n",
    "    # calculate the number of people per role\n",
    "    pt = v[['Grant.Application.ID','Role']].groupby(['Grant.Application.ID', 'Role']).\\\n",
    "        agg(num = ('Role','count')).reset_index().\\\n",
    "        pivot(index = 'Grant.Application.ID',columns = 'Role').reset_index()\n",
    "    newcolname = [x[0]+shortNames[x[1]] for x in pt.columns]\n",
    "    pt.columns = newcolname\n",
    "    pt = pt.fillna(0)\n",
    "    investCount = noZV(allID.merge(pt, on = ['Grant.Application.ID'], how = 'left'))\n",
    "    \n",
    "    # for each role, calculate the frequency of people in each age group\n",
    "    x = v[['Grant.Application.ID','Role','Year.of.Birth']].\\\n",
    "        groupby(['Grant.Application.ID','Role','Year.of.Birth']).\\\n",
    "        agg(num = ('Grant.Application.ID','count')).reset_index()\n",
    "    x = x.sort_values(['Year.of.Birth','Role'])\n",
    "    x['roleC'] = x['Role'].map(shortNames,na_action = 'ignore')+\".\"+x['Year.of.Birth'].astype(int).astype(str)\n",
    "    pt = x.pivot(index = 'Grant.Application.ID',columns = 'roleC',values = 'num').fillna(0).reset_index()\n",
    "    pt.columns.names = [None]\n",
    "    investDOB = noZV(allID.merge(pt, on = ['Grant.Application.ID'], how = 'left'))\n",
    "    \n",
    "    # for each role, calculate the frequency of people from each country\n",
    "    x = v[['Grant.Application.ID','Role','Country.of.Birth']].\\\n",
    "        groupby(['Grant.Application.ID','Role','Country.of.Birth']).\\\n",
    "        agg(num = ('Grant.Application.ID','count')).reset_index()\n",
    "    x = x.sort_values(['Country.of.Birth','Role'])\n",
    "    x['roleC'] = x['Role'].map(shortNames,na_action = 'ignore')+\".\"+x['Country.of.Birth']\n",
    "    pt = x.pivot(index = 'Grant.Application.ID',columns = 'roleC',values = 'num').fillna(0).reset_index()\n",
    "    pt.columns.names = [None]\n",
    "    investCountry = noZV(allID.merge(pt, on = ['Grant.Application.ID'], how = 'left'))\n",
    "    \n",
    "    # for each role, calculate the frenquency of people for each language\n",
    "    x = v[['Grant.Application.ID','Role','Home.Language']].\\\n",
    "        groupby(['Grant.Application.ID','Role','Home.Language']).\\\n",
    "        agg(num = ('Grant.Application.ID','count')).reset_index()\n",
    "    x = x.sort_values(['Home.Language','Role'])\n",
    "    x['roleC'] = x['Role'].map(shortNames,na_action = 'ignore')+\".\"+x['Home.Language']\n",
    "    pt = x.pivot(index = 'Grant.Application.ID',columns = 'roleC',values = 'num').fillna(0).reset_index()\n",
    "    pt.columns.names = [None]\n",
    "    investLang = noZV(allID.merge(pt, on = ['Grant.Application.ID'], how = 'left').fillna(0))\n",
    "    \n",
    "    # for each role, determine who has a Ph.D\n",
    "    x = v[['Grant.Application.ID','Role','With.PHD']].\\\n",
    "        groupby(['Grant.Application.ID','Role','With.PHD']).\\\n",
    "        agg(num = ('Grant.Application.ID','count')).reset_index()\n",
    "    x = x.sort_values(['With.PHD','Role'])    \n",
    "    x['roleC'] = x['Role'].map(shortNames,na_action = 'ignore')+'.PhD'\n",
    "    pt = x.pivot(index = 'Grant.Application.ID',columns = 'roleC',values = 'num').fillna(0).reset_index()\n",
    "    pt.columns.names = [None]\n",
    "    for x in ['EA.PhD','SCI.PhD','UNK.PhD']:\n",
    "        pt[x]=0\n",
    "    investPhD = noZV(allID.merge(pt, on = ['Grant.Application.ID'], how = 'left'))\n",
    "    \n",
    "    # for each role, calculate the number of successful and unsuccessful grants\n",
    "    x = v[['Grant.Application.ID','Role','Number.of.Successful.Grant']].\\\n",
    "        groupby(['Grant.Application.ID','Role']).\\\n",
    "        agg(num = ('Number.of.Successful.Grant','sum')).reset_index()\n",
    "    x = x.sort_values(['Role'])\n",
    "    x['roleC'] = 'Success.'+x['Role'].map(shortNames,na_action = 'ignore')\n",
    "    \n",
    "    y = v[['Grant.Application.ID','Role','Number.of.Unsuccessful.Grant']].\\\n",
    "        groupby(['Grant.Application.ID','Role']).\\\n",
    "        agg(num = ('Number.of.Unsuccessful.Grant','sum')).reset_index()\n",
    "    y = y.sort_values(['Role'])\n",
    "    y['roleC'] = 'Unsuccess.'+y['Role'].map(shortNames,na_action = 'ignore')\n",
    "    pt = pd.concat([x,y]).pivot(index = 'Grant.Application.ID',columns = 'roleC',values = 'num').fillna(0).reset_index()\n",
    "    pt.columns.names = [None]\n",
    "    investGrants = noZV(allID.merge(pt, on = ['Grant.Application.ID'], how = 'left'))\n",
    "    \n",
    "    # Create variables for each role/department combination\n",
    "    x = v[['Grant.Application.ID','Role','Dept.No.']].\\\n",
    "        groupby(['Grant.Application.ID','Role','Dept.No.']).\\\n",
    "        agg(num = ('Grant.Application.ID','count')).reset_index()\n",
    "    x = x.sort_values(['Dept.No.','Role'])\n",
    "    x['roleC'] = x['Role'].map(shortNames,na_action = 'ignore')+\".\"+x['Dept.No.']\n",
    "    pt = x.pivot(index = 'Grant.Application.ID',columns = 'roleC',values = 'num').fillna(0).reset_index()\n",
    "    pt.columns.names = [None]\n",
    "    pt = pt[[x for x in pt.columns if 'DeptNA' not in x]]\n",
    "    investDept = noZV(allID.merge(pt, on = ['Grant.Application.ID'], how = 'left'))\n",
    "    \n",
    "    # Create variables for each role/faculty\n",
    "    x = v[['Grant.Application.ID','Role','Faculty.No.']].\\\n",
    "        groupby(['Grant.Application.ID','Role','Faculty.No.']).\\\n",
    "        agg(num = ('Grant.Application.ID','count')).reset_index()\n",
    "    x = x.sort_values(['Faculty.No.','Role'])\n",
    "    x['roleC'] = x['Role'].map(shortNames,na_action = 'ignore')+\".\"+x['Faculty.No.']\n",
    "    pt = x.pivot(index = 'Grant.Application.ID',columns = 'roleC',values = 'num').fillna(0).reset_index()\n",
    "    pt.columns.names = [None]    \n",
    "    pt = pt[[x for x in pt.columns if 'NA' not in x]]\n",
    "    investFaculty = noZV(allID.merge(pt, on = ['Grant.Application.ID'], how = 'left'))\n",
    "    \n",
    "    # Create dummy variables for each tenure length\n",
    "    x = v[['Grant.Application.ID','No..of.Years.in.Uni.at.Time.of.Grant']].\\\n",
    "        groupby(['Grant.Application.ID','No..of.Years.in.Uni.at.Time.of.Grant']).\\\n",
    "        agg(num = ('Grant.Application.ID','count')).reset_index()\n",
    "    pt = x.pivot(index = 'Grant.Application.ID', columns = 'No..of.Years.in.Uni.at.Time.of.Grant', values = 'num').fillna(0)\n",
    "    pt.columns.names = [None]\n",
    "    investDuration = noZV(allID.merge(pt, on = ['Grant.Application.ID'], how = 'left'))\n",
    "    \n",
    "    # Create variables for the number of publications per journal type. \n",
    "    # Note that we also compute the total number, \n",
    "    # which should be removed for models that cannot deal with such a linear dependency\n",
    "    x = v[['Grant.Application.ID','A.','A','B','C']].\\\n",
    "    groupby('Grant.Application.ID').\\\n",
    "    agg(AstarTotal = ('A.','sum'), ATotal=('A','sum'),BTotal = ('B', 'sum'), CTotal = ('C','sum')).reset_index().fillna(0)\n",
    "    x['allPub'] = x['AstarTotal']+x['ATotal']+x['BTotal']+x['CTotal']\n",
    "    totalPub = x\n",
    "    \n",
    "    # Create variables for the number of publications per journal type per role.\n",
    "    x = v[['Grant.Application.ID','Role','A.','A','B','C']].rename(columns={'A.':'Astar'}).\\\n",
    "        groupby(['Grant.Application.ID','Role']).\\\n",
    "        agg({'Astar':'sum','A':'sum', 'B':'sum','C':'sum'}).reset_index()\n",
    "    pt = x.pivot(index='Grant.Application.ID', columns = 'Role', values = ['Astar','A','B','C']).fillna(0)\n",
    "    newColNames = [x[0]+'.'+shortNames[x[1]] for x in pt.columns]\n",
    "    pt.columns = newColNames\n",
    "    pt = pt.reset_index()\n",
    "    investPub = noZV(allID.merge(pt, on = ['Grant.Application.ID'], how = 'left'))\n",
    "    \n",
    "    # Create variables for each RFCD code\n",
    "    x = v[['Grant.Application.ID','RFCD.Code']].\\\n",
    "        groupby(['Grant.Application.ID','RFCD.Code']).\\\n",
    "        agg(num = ('Grant.Application.ID','count')).reset_index()\n",
    "    pt = x.pivot(index='Grant.Application.ID', columns = 'RFCD.Code',values = 'num').reset_index().drop(columns = ['RFCDNA']).fillna(0)\n",
    "    pt.columns.names=[None]\n",
    "    RFCDcount = noZV(allID.merge(pt, on = ['Grant.Application.ID'], how = 'left'))\n",
    "\n",
    "    # Create variables for each SEO code\n",
    "    x = v[['Grant.Application.ID','SEO.Code']].\\\n",
    "        groupby(['Grant.Application.ID','SEO.Code']).\\\n",
    "        agg(num = ('Grant.Application.ID','count')).reset_index()\n",
    "    pt = x.pivot(index='Grant.Application.ID', columns = 'SEO.Code',values = 'num').reset_index().fillna(0).drop(columns = ['SEONA'])\n",
    "    pt.columns.names=[None]\n",
    "    SEOcount = noZV(allID.merge(pt, on = ['Grant.Application.ID'], how = 'left'))    \n",
    "    \n",
    "    # Create the grantData\n",
    "    x = raw[[\"Sponsor.Code\", \"Contract.Value.Band...see.note.A\", \"Grant.Category.Code\"]]\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    startTime = raw['Start.date'].apply(lambda x : datetime.strptime(x,'%d/%m/%y'))\n",
    "    startYear = startTime.apply(lambda x: x.year)\n",
    "\n",
    "    mthabbre=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "    wdayabbre = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']\n",
    "    x['Month'] = startTime.apply(lambda x: mthabbre[x.month-1])\n",
    "    x['Day'] = startTime.apply(lambda x: x.day)\n",
    "    x['Weekday']=startTime.apply(lambda x: wdayabbre[x.dayofweek])\n",
    "\n",
    "    y = enc.fit_transform(np.array(x[['Sponsor.Code', \"Contract.Value.Band...see.note.A\", \"Grant.Category.Code\",'Month','Weekday']]))\n",
    "    y = pd.DataFrame(data = y.toarray(),columns = enc.get_feature_names_out())\n",
    "    y = y.rename(columns = dict(zip(list(y.columns), [x[3:] for x in y.columns])))\n",
    "    y['Day'] = x['Day']\n",
    "    y['Grant.Application.ID'] = raw['Grant.Application.ID']\n",
    "    y['Class'] = raw['Grant.Status'].map({0:'unsuccessful', 1:'successful'})\n",
    "    y['is2008'] = startYear == 2008\n",
    "    y.columns = [a.strip() for a in y.columns]\n",
    "    grantData = noZV(y)\n",
    "    \n",
    "    # Merge all the predictors together, remove zero variance columns and merge in the outcome data\n",
    "    summarized = investCount\n",
    "    for x in [investDOB, investCountry, investLang, investPhD, investGrants, \\\n",
    "        investDept, investFaculty, investDuration, investPub, totalPub, people, RFCDcount, SEOcount, grantData]:\n",
    "        summarized = summarized.merge(x, on = ['Grant.Application.ID'], how = 'left')\n",
    "    \n",
    "    return investCount, investDOB, investCountry, investLang, investPhD, investGrants, \\\n",
    "        investDept, investFaculty, investDuration, investPub, totalPub, people, RFCDcount, SEOcount, grantData, summarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eff5364d-fd72-43a9-8c28-f664738f3fa9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def checkVerticalData(v):\n",
    "    vertical_filename='vertical.csv'\n",
    "    vertical_path = os.path.join(unimelbdatafolder, vertical_filename)\n",
    "\n",
    "    v0 = pd.read_csv(vertical_path)\n",
    "    v0['RFCD.Code'] = v0['RFCD.Code'].fillna('RFCDNA')\n",
    "    v0['SEO.Code'] = v0['SEO.Code'].fillna('SEONA')\n",
    "    v0['Dept.No.'] = v0['Dept.No.'].fillna('DeptNA')\n",
    "    v0['Faculty.No.'] = v0['Faculty.No.'].fillna('FacultyNA')\n",
    "\n",
    "    v0s = v0.sort_values(['Grant.Application.ID','RFCD.Code']).reset_index(drop=True)\n",
    "    vs = v.sort_values(['Grant.Application.ID','RFCD.Code']).reset_index(drop=True)\n",
    "    return v0s.equals(vs)\n",
    "\n",
    "def checkSummaryData(summary):\n",
    "    s = summary[-1].copy()\n",
    "    s.columns = [x.lower() for x in s.columns]\n",
    "    s =s.drop(columns = ['grant.application.id'])\n",
    "\n",
    "    summary_filename='summarized.csv'\n",
    "    summary_path = os.path.join(unimelbdatafolder, summary_filename)\n",
    "    s0 = pd.read_csv(summary_path)\n",
    "    s0.columns = [x.lower() for x in s0.columns]\n",
    "\n",
    "    cm = s[list(s0.columns)]==(s0)\n",
    "    diffcol = []\n",
    "    for x in s.columns:\n",
    "        if (len(cm[x].value_counts())!=1):\n",
    "            diffcol.append(x)\n",
    "\n",
    "    # Check columns with PhD information: differ by NaN values\n",
    "    p = summary[4].copy()\n",
    "    p.columns = [x.lower() for x in p.columns]\n",
    "\n",
    "    PHD_filename='investPhD.csv'\n",
    "    PHD_path = os.path.join(unimelbdatafolder, PHD_filename)\n",
    "\n",
    "    p0 = pd.read_csv(PHD_path)\n",
    "    p0.columns = [x.lower() for x in p0.columns]\n",
    "    p0 = p0[list(p.columns)]\n",
    "\n",
    "    p = p.fillna('Missing')\n",
    "    p0 = p0.fillna('Missing')\n",
    "\n",
    "    # Check country columns: fill nan by 0 in getSummaryData\n",
    "    c = summary[2].copy()\n",
    "    c.columns = [x.lower() for x in c.columns]\n",
    "    for x in c.columns:\n",
    "        c[x]=c[x].fillna(0).astype('int64')\n",
    "\n",
    "    Country_filename='investCountry.csv'\n",
    "    Country_path = os.path.join(unimelbdatafolder, Country_filename)\n",
    "\n",
    "    c0 = pd.read_csv(Country_path)\n",
    "    c0.columns = [x.lower() for x in c0.columns]\n",
    "    c0 = c0[list(c.columns)]\n",
    "\n",
    "    # Check DOB columns: fill nan by 0 in getSummaryData\n",
    "    b = summary[1].copy()\n",
    "    b.columns = [x.lower() for x in b.columns]\n",
    "    for x in b.columns:\n",
    "        b[x]=b[x].fillna(0).astype('int64')\n",
    "\n",
    "    DOB_filename='investDOB.csv'\n",
    "    DOB_path = os.path.join(unimelbdatafolder, DOB_filename)\n",
    "\n",
    "    b0 = pd.read_csv(DOB_path)\n",
    "    b0.columns = [x.lower() for x in b0.columns]\n",
    "    b0 = b0[list(b.columns)]\n",
    "\n",
    "    return \"Columns in Country and Date.of.Birth needs to fill null values by 0\", {'PhD':p0.equals(p), 'Country': c0.equals(c), 'Date.of.Birth':b0.equals(b)}, \\\n",
    "    [x for x in list(c.columns)+list(b.columns) if x.lower()!='grant.application.id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9019b4c5-3e79-4acd-a755-46747fbb7eff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getTrainTestData(s):\n",
    "    summarized = s.copy()\n",
    "    col1 = [x for x in summarized.columns if x.lower() =='is2008'][0]\n",
    "    col2 = [x for x in summarized.columns if x.lower() =='grant.application.id'][0]\n",
    "    \n",
    "    training = summarized[~summarized[col1]]\n",
    "    year2008= summarized[summarized[col1]]\n",
    "\n",
    "    np.random.seed(552)\n",
    "    inTrain = np.random.choice(year2008.index, size = 1557, replace = False)\n",
    "\n",
    "    training2 = year2008[year2008.index.isin(inTrain)]\n",
    "    testing = year2008[~year2008.index.isin(inTrain)]\n",
    "    training = pd.concat([training, training2])\n",
    "\n",
    "    training = training.drop(columns = [col1,col2])\n",
    "    testing = testing.drop(columns = [col1,col2])\n",
    "\n",
    "    training = noZV(training)\n",
    "    testing = testing[list(training.columns)]\n",
    "    year2008 = year2008[list(training.columns)]\n",
    "    return training, testing, year2008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "496240cb-e291-4ed7-beae-447fb5dbbeeb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def removeHighCorrColumns(XTrain, threshold = 0.99):\n",
    "    corr_df = XTrain.corr().fillna(0)\n",
    "    for i in range(len(corr_df)):\n",
    "        corr_df.iloc[i,i] = 0\n",
    "    abs_corr = np.abs(np.array(corr_df))\n",
    "    columnnames = list(corr_df.columns)\n",
    "\n",
    "    while abs_corr.max() > threshold:\n",
    "        i, j = np.unravel_index(abs_corr.argmax(), abs_corr.shape)\n",
    "        #print(i,j, abs_corr.max())\n",
    "        if abs_corr[i,].mean()> abs_corr[j,].mean():\n",
    "            k = i\n",
    "        else:\n",
    "            k = j\n",
    "        columnnames.pop(k)\n",
    "        abs_corr = np.delete(abs_corr, k, axis = 0)\n",
    "        abs_corr = np.delete(abs_corr, k, axis = 1)\n",
    "    return columnnames\n",
    "\n",
    "def nearZeroVar(X, freqCut):\n",
    "    columnnames = list(X.columns)\n",
    "    percentUnique, freqRatio, zeroVar, nzv = [], [], [], []\n",
    "    for c in columnnames:\n",
    "        w = X[c]\n",
    "        valcnt = w.value_counts().values\n",
    "        percentUnique.append(100* len(valcnt)/valcnt.sum())\n",
    "        zeroVar.append(True if len(valcnt)==1 else False)\n",
    "        if len(valcnt)==1: \n",
    "            freqRatio.append(0)\n",
    "        else:\n",
    "            freqRatio.append(valcnt[0]/valcnt[1])\n",
    "        nzv.append(freqRatio[-1]==0 or freqRatio[-1]>freqCut)\n",
    "    df = pd.DataFrame({'col':columnnames, 'freqRatio':freqRatio, 'zeroVar':zeroVar,'nzv':nzv}).set_index('col')\n",
    "    df.index.name = None\n",
    "    return df\n",
    "\n",
    "def getFullSetReducedSet(training):\n",
    "    fullSet = [x for x in training.columns if x.lower()!='class']\n",
    "    XTrain = training[fullSet]\n",
    "    col1 = removeHighCorrColumns(XTrain)\n",
    "    fullSet = col1\n",
    "\n",
    "    isNZV = nearZeroVar(training[fullSet], int(len(training)/5))\n",
    "    fullSet = list(isNZV[~isNZV['nzv']].index)\n",
    "\n",
    "    reducedSet = list(isNZV[(~isNZV['nzv']) & (isNZV['freqRatio']<int(len(training)/50))].index)\n",
    "    reducedSet = [x for x in reducedSet if x.lower() not in ['allpub','numpeople','mar','sun']]\n",
    "    return fullSet, reducedSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571291b8-6ee6-4c19-a61f-9b73c9a7ed76",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Read csv file and get summary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57d2b1dd-4058-4627-a491-bead6945bb2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8708 entries, 0 to 8707\n",
      "Columns: 251 entries, Grant.Application.ID to C.15\n",
      "dtypes: float64(179), int64(2), object(70)\n",
      "memory usage: 16.7+ MB\n"
     ]
    }
   ],
   "source": [
    "raw = pd.read_csv(trainfile_path, low_memory=False)\n",
    "raw = raw.drop(columns = ['Unnamed: 251'])\n",
    "raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53456b0b-f535-448e-92ba-39ceaeada36b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "namesPre = []\n",
    "int1to15 = [str(x) for x in range(1,16)]\n",
    "for x in raw.columns:\n",
    "    if x[x.rfind(\".\")+1:] in int1to15:\n",
    "        if x[:x.rfind(\".\")] not in namesPre:\n",
    "            namesPre.append(x[:x.rfind(\".\")])\n",
    "raw = cleanRawData(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3ef5bc3-e7e8-4962-8404-da527f44c12c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vertical = getVerticalData(raw,namesPre)\n",
    "vertical =cleanVerticalData(vertical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "786839f9-789b-41a6-ad16-b2dc499b6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = getSummaryData(vertical)\n",
    "m, dic, l = checkSummaryData(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69779132-526a-4454-bcc9-4a03b4697e75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarized = s[-1]\n",
    "fillList = [x for x in summarized.columns if x.lower() in l]\n",
    "summarized[fillList] = summarized[fillList].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "54b84834-4076-46a2-9fac-2e04d51c868c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training, testing, year2008Data = getTrainTestData(summarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "242f6c3b-2b45-4768-88c3-ef90d1864b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fullSet, reducedSet = getFullSetReducedSet(training)\n",
    "pd.DataFrame({'colName':fullSet}).to_csv(\"./fullset.csv\",index=False)\n",
    "pd.DataFrame({'colName':reducedSet}).to_csv(\"./reducedset.csv\",index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "023cb499-5598-487c-87af-3b658c4ddd38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8190, 1893), (518, 1893), (2075, 1893))"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(training).shape, np.array(testing).shape, np.array(year2008Data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "2dc8c100-6c64-4832-8564-5dcabeda6d7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre2008Data = training[~training.index.isin(year2008Data.index)]\n",
    "test2008 = testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "4992cec4-ac63-4b25-9bf8-1124fca62fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "allData=training\n",
    "holdout2008 = testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "57ff7eea-800d-42bd-b4ac-84bc8207bae9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6633, 1893), (518, 1893), (8190, 1893), (518, 1893))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pre2008Data).shape, np.array(test2008).shape, np.array(allData).shape, np.array(holdout2008).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "41b96016-3442-4c82-93d0-7f5cb7fde8b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fullSet = pd.read_csv(\"./fullset.csv\")['colName'].to_list()\n",
    "reducedSet = pd.read_csv(\"./reducedset.csv\")['colName'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "eb87a5d4-f681-417a-a0e7-07a088ec37ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1069, 255)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fullSet), len(reducedSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174b6fc-78ca-42ec-949e-0cffdb9b6c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
